# Start multiple ec2 instances with instance-specific tags assigned to each
#
- hosts: localhost
  vars:

  # Master customization
    master_vm_type: "t2.micro"                                  # General Purpose: 1 vCPU, 1 GB Memory
    #master_vm_type: "m4.xlarge"                                 # General Purpose: 4 vCPU, 16 GB Memory
    master_volume_size: 100                                     # 100 GB

  # Slave customization
    slave_vm_type: "t2.micro"                                   # General Purpose: 1 vCPU, 1 GB Memory
    #slave_vm_type: "m4.4xlarge"                                 # General Purpose: 16 vCPU, 64 GB Memory
    slave_volume_size: 400                                      # 400 GB

  # Common properties
    aws_zone: "us-east-1"                                       # North Virginia
    ami_image: "ami-00a11e68"                                   # RHEL-6.5_GA_HVM-20140929-x86_64-11-Hourly2-GP2
    security_groups: ['default', 'Cloudera-Tuce-New-SG']
    network_id: "vpc-5a49ca3e"
    subnet_id: "subnet-a0d6d48b"
    ssh_user: "ec2-user"
    key_pair_name: "tuce-test"

  # Default values
    default_num_slaves: 2
    default_domain: "atigeo.com"
    default_master_name: "Cloudera Master"
    default_master_hostname: "spark-master"
    default_slave_name: "Cloudera Slave"
    default_slave_hostname: "spark-slave"
    default_environment: "Test Environment"
    default_client: "Atigeo Internal"

  tasks:
    - name: Launch Master instance
      ec2:
        aws_access_key: "{{ aws_access_key|default(lookup('env', 'AWS_ACCESS_KEY')) }}"
        aws_secret_key: "{{ aws_secret_key|default(lookup('env', 'AWS_SECRET_KEY')) }}"
        region: "{{ aws_zone }}"
        instance_type: "{{ master_vm_type }}"
        image: "{{ ami_image }}"
        vpc_subnet_id: "{{ subnet_id }}"
        volumes:
          - device_name: /dev/sda1
            volume_types: "gp2"
            volume_size: "{{ master_volume_size }}"
        wait: true
        keypair: "{{ key_pair_name }}"
        groups: "{{ security_groups }}"
        instance_tags:
          Name: "{{ default_master_name }}"
          Environment: "{{ cld_environment|default( default_environment ) }}"
          Client: "{{ cld_client|default( default_client ) }}"
        user_data: |
          #cloud-config
          hostname: {{ default_master_hostname }}
          fqdn: {{ default_master_hostname}}.{{default_domain}}
      register: ec2_master

    - name: Launch Slave instances
      ec2:
        aws_access_key: "{{ aws_access_key|default(lookup('env', 'AWS_ACCESS_KEY')) }}"
        aws_secret_key: "{{ aws_secret_key|default(lookup('env', 'AWS_SECRET_KEY')) }}"
        region: "{{ aws_zone }}"
        instance_type: "{{ slave_vm_type }}"
        image: "{{ ami_image }}"
        vpc_subnet_id: "{{ subnet_id }}"
        volumes:
          - device_name: /dev/sda1
            volume_types: "gp2"
            volume_size: "{{ slave_volume_size }}"
        wait: true
        keypair: "{{ key_pair_name }}"
        groups: "{{ security_groups }}"
        instance_tags:
          Name: "{{ default_slave_name }} {{ item }}"
          Environment: "{{ cld_environment|default( default_environment ) }}"
          Client: "{{ cld_client|default( default_client ) }}"
        user_data: |
          #cloud-config
          hostname: {{ default_slave_hostname }}{{ item }}
          fqdn: {{ default_slave_hostname}}{{ item }}.{{default_domain}}
      register: ec2_slaves
      with_sequence: count={{ num_slaves | default( default_num_slaves ) }}

    - name: Wait for EC2 instances to boot-up
      pause: seconds=400

    - name: Wait for SSH
      wait_for:
        host: "{{ item.instances.0.private_ip }}"
        port: 22
        timeout: 300
        delay: 30
        state: started
      ignore_errors: yes
      with_items:
        - "{{ ec2_master }}"
        - "{{ ec2_slaves.results }}"

    - name: Wait for SSH
      pause: seconds=30

    - name: Add
      add_host:
        name: "{{ item.instances.0.private_ip }}"
        groups: cluster_hosts
      with_flattened:
        - "{{ ec2_master }}"
        - "{{ ec2_slaves.results }}"

- hosts: cluster_hosts
  tasks:
    - ping:

    - name: Dump all vars
      local_action: template src=templates/debug_dump_all_variables.j2 dest=/tmp/ansible.all

      # Idempotent way to build a /etc/hosts file with Ansible using your Ansible hosts inventory for a source.
      # Will include all hosts the playbook is run on.
      # Inspired from http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html

    - name: "Build hosts file"
      lineinfile: dest=/etc/hosts regexp='.*{{ item }}$' line="{{ hostvars[item].ansible_default_ipv4.address }} {{ hostvars[item].ansible_hostname }} {{ hostvars[item].ansible_hostname }}.{{ default_domain }}" state=present
      when: hostvars[item].ansible_default_ipv4.address is defined
      with_items: "{{ groups['all'] }}"
